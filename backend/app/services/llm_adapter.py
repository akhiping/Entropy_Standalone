"""
LLM Adapter service for interfacing with different language model providers.
"""

from typing import Dict, Any, List, Optional
from app.config import settings

class LLMAdapterService:
    """Service for interfacing with various LLM providers."""
    
    def __init__(self):
        self.provider = settings.DEFAULT_LLM_PROVIDER
        self.model = settings.DEFAULT_MODEL
        
        # In development mode without API keys, use mock responses
        if not settings.OPENAI_API_KEY and self.provider == "openai":
            print("ðŸš« Using mock LLM responses - OpenAI API key not available")
            self.use_mock = True
        elif not settings.ANTHROPIC_API_KEY and self.provider == "anthropic":
            print("ðŸš« Using mock LLM responses - Anthropic API key not available")
            self.use_mock = True
        else:
            self.use_mock = False
    
    async def generate_response(
        self, 
        prompt: str, 
        context: Optional[str] = None,
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> Dict[str, Any]:
        """Generate a response from the LLM."""
        
        if self.use_mock:
            # Return a mock response for testing
            return {
                "response": f"This is a mock response to: '{prompt[:50]}...' "
                           f"Context provided: {bool(context)}. "
                           f"In a real implementation, this would be generated by {self.provider}.",
                "provider": self.provider,
                "model": self.model,
                "tokens_used": 50,
                "mock": True
            }
        
        # TODO: Implement actual LLM provider calls
        raise NotImplementedError("Real LLM generation not yet implemented")
    
    async def generate_streaming_response(
        self,
        prompt: str,
        context: Optional[str] = None,
        max_tokens: int = 1000,
        temperature: float = 0.7
    ):
        """Generate a streaming response from the LLM."""
        
        if self.use_mock:
            # Mock streaming response
            words = [
                "This", "is", "a", "mock", "streaming", "response", "to", "your", 
                "query", "about", prompt[:20], "...", "Each", "word", "comes", 
                "as", "a", "separate", "chunk", "to", "simulate", "real", "streaming."
            ]
            
            for word in words:
                yield {
                    "chunk": word + " ",
                    "done": False
                }
            
            yield {
                "chunk": "",
                "done": True,
                "total_tokens": len(words)
            }
            return
        
        # TODO: Implement actual streaming LLM calls
        raise NotImplementedError("Real streaming LLM generation not yet implemented")
    
    def is_available(self) -> bool:
        """Check if the LLM service is available."""
        if self.use_mock:
            return True
        
        # TODO: Implement actual availability check
        return False 